{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importando librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.request import FancyURLopener\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from random import choice\n",
    "from urllib.error import HTTPError \n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción y Preparación de la Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrayendo la data para análisis\n",
    "#NOTA: PARA JALAR LA DATA DE OTRA PC, CAMBIAR LA RUTA DE s A LA NUEVA RUTA\n",
    "s=r'C:\\Users\\Jorge Pablo\\Documents\\PUCP\\II_MineriaWeb\\Trabajo\\FakeNewCorpusSpanish_V1\\train.xlsx'\n",
    "archivo=pd.ExcelFile(s)\n",
    "hoja=archivo.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Source</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fake</td>\n",
       "      <td>Education</td>\n",
       "      <td>El Ruinaversal</td>\n",
       "      <td>RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...</td>\n",
       "      <td>RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...</td>\n",
       "      <td>http://www.elruinaversal.com/2017/06/10/rae-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fake</td>\n",
       "      <td>Education</td>\n",
       "      <td>Hay noticia</td>\n",
       "      <td>La palabra \"haiga\", aceptada por la RAE</td>\n",
       "      <td>La palabra \"haiga\", aceptada por la RAE La Rea...</td>\n",
       "      <td>https://haynoticia.es/la-palabra-haiga-aceptad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fake</td>\n",
       "      <td>Education</td>\n",
       "      <td>El Ruinaversal</td>\n",
       "      <td>YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...</td>\n",
       "      <td>YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...</td>\n",
       "      <td>http://www.elruinaversal.com/2018/05/06/yordi-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>Education</td>\n",
       "      <td>EL UNIVERSAL</td>\n",
       "      <td>UNAM capacitará a maestros para aprobar prueba...</td>\n",
       "      <td>UNAM capacitará a maestros para aprobar prueba...</td>\n",
       "      <td>http://www.eluniversal.com.mx/articulo/nacion/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fake</td>\n",
       "      <td>Education</td>\n",
       "      <td>Lamula</td>\n",
       "      <td>pretenden aprobar libros escolares con conteni...</td>\n",
       "      <td>Alerta: pretenden aprobar libros escolares con...</td>\n",
       "      <td>https://redaccion.lamula.pe/2018/06/19/memoria...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category      Topic          Source  \\\n",
       "0     Fake  Education  El Ruinaversal   \n",
       "1     Fake  Education     Hay noticia   \n",
       "2     Fake  Education  El Ruinaversal   \n",
       "3     True  Education    EL UNIVERSAL   \n",
       "4     Fake  Education          Lamula   \n",
       "\n",
       "                                            Headline  \\\n",
       "0  RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...   \n",
       "1            La palabra \"haiga\", aceptada por la RAE   \n",
       "2  YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...   \n",
       "3  UNAM capacitará a maestros para aprobar prueba...   \n",
       "4  pretenden aprobar libros escolares con conteni...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  RAE INCLUIRÁ LA PALABRA \"LADY\" EN EL DICCIONAR...   \n",
       "1  La palabra \"haiga\", aceptada por la RAE La Rea...   \n",
       "2  YORDI ROSADO ESCRIBIRÁ Y DISEÑARÁ LOS NUEVOS L...   \n",
       "3  UNAM capacitará a maestros para aprobar prueba...   \n",
       "4  Alerta: pretenden aprobar libros escolares con...   \n",
       "\n",
       "                                                Link  \n",
       "0  http://www.elruinaversal.com/2017/06/10/rae-in...  \n",
       "1  https://haynoticia.es/la-palabra-haiga-aceptad...  \n",
       "2  http://www.elruinaversal.com/2018/05/06/yordi-...  \n",
       "3  http://www.eluniversal.com.mx/articulo/nacion/...  \n",
       "4  https://redaccion.lamula.pe/2018/06/19/memoria...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=archivo.parse('Hoja de datos')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(676, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtener solo las paginas http\n",
    "df_link=df.drop(['Category','Topic','Source','Headline','Text'] ,axis=1)\n",
    "#Convirtiendo la data http de tipo objeto a string para uso en URLopener\n",
    "df_link=df_link['Link'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.eluniversal.com.mx/articulo/nacion/sociedad/2017/02/8/unam-capacitara-maestros-para-aprobar-prueba-pisa'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prueba del link\n",
    "textp=df_link.loc[3]\n",
    "textp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colocando distintos agentes de búsqueda\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows; U; Windows NT 5.1; it; rv:1.8.1.11) Gecko/20071127 Firefox/2.0.0.11',\n",
    "    'Opera/9.25 (Windows NT 5.1; U; en)',\n",
    "    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)',\n",
    "    #'Mozilla/5.0 (compatible; Konqueror/3.5; Linux) KHTML/3.5.5 (like Gecko) (Kubuntu)',\n",
    "    #'Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.8.0.12) Gecko/20070731 Ubuntu/dapper-security Firefox/1.5.0.12',\n",
    "    #'Lynx/2.8.5rel.1 libwww-FM/2.14 SSL-MM/1.4.1 GNUTLS/1.2.9'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funcion de extraccion total no se usa FancyURLopener porque bloqueaba la descarg\n",
    "Se cambia de urlpone y urllib.request\n",
    "Se agrega excepción para errores de paginas http inexistentes, se coloca \"NA\" en esos campos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agrupa_textos(df_link,rango):\n",
    "    grupo=[]\n",
    "    #class MyOpener(FancyURLopener,object):\n",
    "        #version = choice(user_agents)\n",
    "    #myopener=MyOpener()\n",
    "    agent=choice(user_agents)\n",
    "    \n",
    "    head = {'User-Agent': agent}\n",
    "\n",
    "    \n",
    "    for i in range(0,rango):\n",
    "                  \n",
    "        texto= df_link_part1.loc[i]\n",
    "        try:\n",
    "            html = urlopen(texto)\n",
    "        except:\n",
    "            ptag=\"NA\"\n",
    "        else:\n",
    "            #texto_html=myopener.open(urllib.parse.unquote(texto)).read()\n",
    "            req  =urllib.request.Request(texto, data=None,headers=head)\n",
    "            response = urllib.request.urlopen(req)\n",
    "            texto_html=response.read()\n",
    "            soup=BeautifulSoup(texto_html,\"lxml\")\n",
    "            ptag=soup.findAll('p')\n",
    "        grupo.append(ptag)\n",
    "           \n",
    "              \n",
    "    return(grupo)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicalizar el grupo, obtener la cantidad de descarga\n",
    "grupo2=[]\n",
    "total=len(df_link)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llamada a descarga de archivo de entrenamiento, se realizan descargas por lotes de 10 con intervalos de 1 minuto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10\n",
      "1    20\n",
      "2    30\n",
      "3    40\n",
      "4    50\n",
      "5    60\n",
      "6    70\n",
      "7    80\n",
      "8    90\n",
      "9    100\n",
      "10    110\n",
      "11    120\n",
      "12    130\n",
      "13    140\n",
      "14    150\n",
      "15    160\n",
      "16    170\n",
      "17    180\n",
      "18    190\n",
      "19    200\n",
      "20    210\n",
      "21    220\n",
      "22    230\n",
      "23    240\n",
      "24    250\n",
      "25    260\n",
      "26    270\n",
      "27    280\n",
      "28    290\n",
      "29    300\n",
      "30    310\n",
      "31    320\n",
      "32    330\n",
      "33    340\n",
      "34    350\n",
      "35    360\n",
      "36    370\n",
      "37    380\n",
      "38    390\n",
      "39    400\n",
      "40    410\n",
      "41    420\n",
      "42    430\n",
      "43    440\n",
      "44    450\n",
      "45    460\n",
      "46    470\n",
      "47    480\n",
      "48    490\n",
      "49    500\n",
      "50    510\n",
      "51    520\n",
      "52    530\n",
      "53    540\n",
      "54    550\n",
      "55    560\n",
      "56    570\n",
      "57    580\n",
      "58    590\n",
      "59    600\n",
      "60    610\n",
      "61    620\n",
      "62    630\n",
      "63    640\n",
      "64    650\n",
      "65    660\n",
      "66    670\n"
     ]
    }
   ],
   "source": [
    "parcial=int(total/10)\n",
    "inicio=0\n",
    "for i in range(0,parcial):\n",
    "    cantidad=inicio+10\n",
    "    df_link_part1 = df_link.iloc[inicio:cantidad]\n",
    "    df_link_part1 = df_link_part1.reset_index(drop=True)\n",
    "    \n",
    "    rango=cantidad-inicio\n",
    "    \n",
    "    grupo=agrupa_textos(df_link_part1,rango)\n",
    "    for s in range(0,rango):\n",
    "        grupo2.append(grupo[s])\n",
    "    inicio=inicio+rango\n",
    "    time.sleep(60)\n",
    "    \n",
    "    print(i,\"  \",len(grupo2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversión del Texto a Base de Datos. Formalmente se convierte en Filas con 83 columnas de separacón\n",
    "Se entregan dos dataframes: El de 83 columnas y otro de 1 sola columna con Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos_train=pd.DataFrame(grupo2)\n",
    "df_textos_train.shape\n",
    "df_texto2=pd.DataFrame(columns=['textos'])\n",
    "df_texto2['textos'] = df_textos_train.apply(lambda x: ','.join(set(x.astype(str))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probando la data\n",
    "print(df_texto2.loc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer Entregable: Archivos en csv con los textos de las paginas html descargadas. \n",
    "Segunda parte: Limpieza:\n",
    "1.- juntar Columna de Verificación (Noticia Real, Noticia Falsa), eliminar filas con https errados\n",
    "2.- Retirar textos de formato y de indicación fuera del contexto de la notica\n",
    "3.- Eliminar artículos y otras palabras irrelevantes para la vectorización. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_textos_train.to_csv(r'C:\\Users\\Jorge Pablo\\Documents\\PUCP\\II_MineriaWeb\\Trabajo\\FakeNewCorpusSpanish_V1\\textoseparado_train.csv',encoding='utf-8')\n",
    "df_texto2.to_csv(r'C:\\Users\\Jorge Pablo\\Documents\\PUCP\\II_MineriaWeb\\Trabajo\\FakeNewCorpusSpanish_V1\\textojunto_train.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
